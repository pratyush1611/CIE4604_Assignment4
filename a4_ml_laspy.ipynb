{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import laspy\n",
    "from scipy.spatial import cKDTree\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_spatial_subset(\n",
    "    full_cloud, xmin: int, xmax: int, ymin: int, ymax: int\n",
    ") -> laspy.lasdata.LasData:\n",
    "    \"\"\"\n",
    "    Create a subset of a LAS file based on a bounding box in coordinates of the cloud file\n",
    "\n",
    "    parameters:\n",
    "    full_cloud: a laspy cloud object you want to subset from\n",
    "    xmin,xmax,ymin\n",
    "\n",
    "    returns: laspy.lasdata.LasData object\n",
    "    \"\"\"\n",
    "    # create empty laspy collection to put the filtered points in that has the same format and file version as the original\n",
    "    new_file = laspy.create(\n",
    "        point_format=cloud.header.point_format, file_version=cloud.header.version\n",
    "    )\n",
    "    # create matrices of boolean values\n",
    "    a = cloud.x > xmin\n",
    "    b = cloud.x < xmax\n",
    "    c = cloud.y > ymin\n",
    "    d = cloud.y < ymax\n",
    "    # subset the points and put them into the new laspy file\n",
    "    new_file.points = full_cloud.points[a & b & c & d]\n",
    "    return new_file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in the fully LAZ file\n",
    "cloud = laspy.read(r'./data/AHN4_Noordwijk.laz')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bounding box is between 89200.0 and 89600.0 in the x direction and 472668.0 and 473068.0 in the y direction (referenced to the coordinates in the LAZ file)\n"
     ]
    }
   ],
   "source": [
    "# define the area of interest\n",
    "xcenter = 89400\n",
    "bbbox_size = 400\n",
    "ycenter = 472868\n",
    "xmin = xcenter - 0.5*bbbox_size\n",
    "xmax = xcenter + 0.5*bbbox_size\n",
    "ymin = ycenter - 0.5*bbbox_size\n",
    "ymax = ycenter + 0.5*bbbox_size\n",
    "print(f'Bounding box is between {xmin} and {xmax} in the x direction and {ymin} and {ymax} in the y direction (referenced to the coordinates in the LAZ file)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = get_spatial_subset(cloud,xmin,xmax,ymin,ymax)\n",
    "subset.write('.\\data\\subsetted.las')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = np.vstack((subset.X,subset.Y,subset.Z)).transpose()\n",
    "tree = cKDTree(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# going to test this algorithm on a smaller subset Of the subset so it runs faster\n",
    "test_set = subset.points.array[10:2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Deviation', 'Reflectance', 'Amplitude', 'normal']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# setting up the extra dimensions to add back to the las file\n",
    "dimension = laspy.point.format.ExtraBytesParams('normal','float64')\n",
    "subset.add_extra_dim(dimension)\n",
    "list(subset.point_format.extra_dimension_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up an empty array\n",
    "normals = np.zeros(len(test_set))\n",
    "for i,point in enumerate(test_set):\n",
    "    x = point[0]\n",
    "    y = point[1]\n",
    "    z = point[2]\n",
    "    neighbors_distance, neighbors_indices = tree.query((x,y,z),k=8)\n",
    "    neighbors_points = tree.data[neighbors_indices]\n",
    "    # set up pca model\n",
    "    pca = PCA(n_components=3)\n",
    "\n",
    "    pca.fit(neighbors_points)\n",
    "    # get the Z value for the normal\n",
    "    normal_vector = pca.components_.T[2]\n",
    "\n",
    "    angle = np.rad2deg(np.arccos(normal_vector.dot([0,0,1])))\n",
    "    normals[i] = angle\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Investigate the data set: describe each of its columns and assess the spread in values in each column. What is the meaning of each attribute?\n",
    "\n",
    "| Column name | max | min | average | description |\n",
    "| ----------- | --- | --- | ------- | ----------- |\n",
    "|  |  |  |  |  |\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) What are ways to create a subset? Select a suitable subset of the data and visualize it in 3D. Start small, and if computer and software permits, try a bit larger subset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Identify at least four different object classes. Choose your classes such that together these cover a large majority of your points.\n",
    "\n",
    "1. building\n",
    "2. grass\n",
    "3. pavement\n",
    "4. tree\n",
    "5. beach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d) Analyse which spatial scales and spectral properties are useful to distinguish your classes.\n",
    "\n",
    "The following properties are typical of each of the classes\n",
    "\n",
    "1. building\n",
    "   1. taller than the surroundings \n",
    "2. grass\n",
    "   1. high NDVI\n",
    "   2. \n",
    "3. pavement\n",
    "   1. \n",
    "4. tree\n",
    "   1. taller that it is wide or long\n",
    "5. beach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e) Extract training data for each of your object classes. Divide your training data in two parts, one for training, and one for validation. What could be the influence of  imbalances in your training data? How could your division in training and validation data affect your results?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "f) Find a suitable implementation of the Random Forest algorithm. What are its parameters? What would be good settings for these parameters, given your classification task? Apply Random Forest on your data using only your observed features, to make sure your setup is correct.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Questions\n",
    "1. Describe the properties of the data (sub)set that you will classify: (i) How many points? (ii) What area does it cover? (iii) What observed features will you use? (iv) Visualize your final subset, including the useful observed features. (vi) Describe your at least four different object classes; For each point determine neighborhoods of k1, k2, ... points (using e.g. an efficient data structure), and use these k1, k2, ... points to estimate several feature values.\n",
    "\n",
    "\n",
    "2. Describe $\\gt$ 20 different geometric attributes, obtained using at least 2 different neighborhood sizes, to characterize your points. What are the dimensions of the data covariance matrix you use to compute the geometric features? Give one example on how you determine the PCA eigenvalues of one k-neighborhood. Indicate for each feature how it could help to distinguish your classes, given also the neighborhood sizes you consider.\n",
    "   \n",
    "\n",
    "3. Compute all geometric features for all points in your subset using Python. Visualize selected results, e.g. by combining features in a false color visualization and/or using histograms. Which features are best at discriminating your classes? Why?\n",
    "\n",
    "4. Describe and visualize your training data. Is your training data balanced? Make sure that a zone of your point cloud data is really ’unseen’, that is that no training data is taken from that zone, so you can inspect if Random Forest also works there.\n",
    "   \n",
    "\n",
    "5. Feed your training data to Random Forest using at least 10 of your best geometric and observed features. What settings did you use?\n",
    "   \n",
    "\n",
    "6. Classify your point cloud data and visualize and discuss your result. What went well? Give also examples where the classifier mixed up classes. What are possible explanations for these confusions? How are the classification results on the unseen zone?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5e58f0493970c85668badbc5bf106a4a788c4b99d38880c90ea927a02d72e3dc"
  },
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit ('BPD4a': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
