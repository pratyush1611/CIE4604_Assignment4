{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data exploration\n",
    "\n",
    "### Contents\n",
    "1. imports\n",
    "1. [load subset](#subsetLoad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import laspy\n",
    "from scipy.spatial import cKDTree\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import proj3d\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Polygon, Point\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 100)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score, RepeatedStratifiedKFold\n",
    "\n",
    "# preprocessing and scoring\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.metrics import (\n",
    "    r2_score,\n",
    "    mean_absolute_error,\n",
    "    mean_squared_error,\n",
    "    mean_absolute_percentage_error,\n",
    ")\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_spatial_subset(\n",
    "    full_cloud, xmin: int, xmax: int, ymin: int, ymax: int\n",
    ") -> laspy.lasdata.LasData:\n",
    "    \"\"\"\n",
    "    Create a subset of a LAS file based on a bounding box in coordinates of the cloud file\n",
    "\n",
    "    parameters:\n",
    "    full_cloud: a laspy cloud object you want to subset from\n",
    "\n",
    "    xmin: the values of the bounding box object\n",
    "    xmax\n",
    "    ymin\n",
    "    ymax\n",
    "\n",
    "    returns: laspy.lasdata.LasData object\n",
    "    \"\"\"\n",
    "\n",
    "    # create empty laspy collection to put the filtered points in that has the same format and file version as the original\n",
    "    new_file = laspy.create(\n",
    "        point_format=cloud.header.point_format, file_version=cloud.header.version,\n",
    "    )\n",
    "\n",
    "    # create matrices of boolean values\n",
    "    a = cloud.x > xmin\n",
    "    b = cloud.x < xmax\n",
    "    c = cloud.y > ymin\n",
    "    d = cloud.y < ymax\n",
    "    # subset the points and put them into the new laspy file\n",
    "    new_file.points = full_cloud.points[a & b & c & d]\n",
    "\n",
    "    new_file.header.offsets = full_cloud.header.offsets\n",
    "    new_file.header.scales = full_cloud.header.scales\n",
    "\n",
    "    return new_file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in the fully LAZ file\n",
    "cloud = laspy.read(r\"./data/AHN4_Noordwijk.laz\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the area of interest\n",
    "xcenter = 89401.42414516  # 89400\n",
    "bbbox_size_length = 170  # 300\n",
    "bbbox_size_width = 100  # 300\n",
    "ycenter = 472877.83174913  # 472868\n",
    "xmin = xcenter - 0.5 * bbbox_size_length\n",
    "xmax = xcenter + 0.5 * bbbox_size_length\n",
    "ymin = ycenter - 0.5 * bbbox_size_width\n",
    "ymax = ycenter + 0.5 * bbbox_size_width\n",
    "\n",
    "# create the subset\n",
    "subset = get_spatial_subset(cloud, xmin, xmax, ymin, ymax)\n",
    "\n",
    "\n",
    "# # saving the bounds as a shapefile\n",
    "\n",
    "lat_point_list = [ymin, ymin, ymax, ymax, ymin] \n",
    "lon_point_list = [xmin, xmax ,xmax, xmin, xmin]\n",
    "\n",
    "polygon_geom = Polygon(zip(lon_point_list, lat_point_list))\n",
    "crs_28992 = {'init':'epsg:28992'}\n",
    "polygon = gpd.GeoDataFrame(index=[0], crs=crs_28992, geometry=[polygon_geom])       \n",
    "print(polygon.geometry)\n",
    "\n",
    "# polygon.to_file(filename='./data/bounds_new.geojson', driver='GeoJSON') # geojson export has a problem with how it is saved, doesnt render in QGIS\n",
    "polygon.to_file(filename='./data/bounds_new.shp', driver=\"ESRI Shapefile\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a tree for all points in the subset\n",
    "dataset = np.vstack((subset.X, subset.Y, subset.Z)).transpose()\n",
    "tree = cKDTree(dataset)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up the extra dimensions to add back to the las file\n",
    "dimension_list = [\n",
    "    laspy.point.format.ExtraBytesParams(\"normal_angle\", \"float64\"),\n",
    "    laspy.point.format.ExtraBytesParams(\"linearity15\", \"float64\"),\n",
    "    laspy.point.format.ExtraBytesParams(\"planarity15\", \"float64\"),\n",
    "    laspy.point.format.ExtraBytesParams(\"scat15\", \"float64\"),\n",
    "    laspy.point.format.ExtraBytesParams(\"curve15\", \"float64\"),\n",
    "]\n",
    "\n",
    "for dim in dimension_list:\n",
    "    subset.add_extra_dim(dim)\n",
    "\n",
    "# check that it was added correctly\n",
    "list(subset.point_format.extra_dimension_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get an array for all the points\n",
    "all_points = subset.points.array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _fit_plane_pca(point, tree, k) -> PCA:\n",
    "    \"\"\"\n",
    "    internal function designed to fit a PCA model to the k nearest points to a point\n",
    "    parameters:\n",
    "    point: single point as structured ndarray of point data returned from laspy.points.array()\n",
    "    tree: the KD tree for the points\n",
    "    k: number of neighbors to consider\n",
    "\n",
    "    returns:\n",
    "    fitted pca model: sklean.decom\n",
    "    \"\"\"\n",
    "    distances, neighbors_indices = tree.query((point[0], point[1], point[2]), k)\n",
    "\n",
    "    neighbors_points = tree.data[neighbors_indices]\n",
    "    # set up pca model\n",
    "    pca = PCA(n_components=3)\n",
    "    # fit the model to the points\n",
    "    pca.fit(neighbors_points)\n",
    "    return pca\n",
    "\n",
    "\n",
    "def _find_angle(vector, axis=\"z\"):\n",
    "    axis_lookup = {\"z\": [0, 0, 1], \"y\": [0, 1, 0], \"x\": [1, 0, 0]}\n",
    "    unit_vector = axis_lookup[axis]\n",
    "    # find the angle with the z axis using the dot product\n",
    "    angle = np.rad2deg(np.arccos(vector.dot(unit_vector)))\n",
    "\n",
    "    # we want all the normals pointing in the same direction\n",
    "\n",
    "    if angle > 90:\n",
    "        # if the vector is pointing the wrong way, flip it\n",
    "        vector = vector * -1\n",
    "        # find the angle of the new vector\n",
    "        angle = np.rad2deg(np.arccos(vector.dot(unit_vector)))\n",
    "\n",
    "    return angle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca_3d_shape(point_array: np.array, tree: cKDTree, k: int = 8) -> np.array:\n",
    "    \"\"\"\n",
    "    takes an array of points from laspy, a kd tree, and an integer number of neighbors to sample\n",
    "\n",
    "\n",
    "    returns an array of:\n",
    "\n",
    "    linearity, planarity, scattering, omnivariance, anisotropy, eigenentropy, eigsum, ch_cur\n",
    "\n",
    "    \"\"\"\n",
    "    length_parray = len(point_array)\n",
    "    normal_angle = np.zeros(length_parray)\n",
    "    normal_vectors = np.zeros((length_parray, 3))\n",
    "    # set up empty arrays\n",
    "    linearity = np.zeros(length_parray)\n",
    "    planarity = np.zeros(length_parray)\n",
    "    scattering = np.zeros(length_parray)\n",
    "    omnivariance = np.zeros(length_parray)\n",
    "    anisotropy = np.zeros(length_parray)\n",
    "    eigenentropy = np.zeros(length_parray)\n",
    "    eigsum = np.zeros(length_parray)\n",
    "    ch_cur = np.zeros(length_parray)\n",
    "    norm_ang_x = np.zeros(length_parray)\n",
    "    norm_ang_y = np.zeros(length_parray)\n",
    "\n",
    "    for i, point in enumerate(point_array):\n",
    "\n",
    "        # get a fitted pca model\n",
    "        pca = _fit_plane_pca(point, tree, k)\n",
    "\n",
    "        # get the Z value for the normal\n",
    "        normal_vector = pca.components_.T[2]\n",
    "\n",
    "        # find the angle with the z axis using the dot product\n",
    "        angle = _find_angle(normal_vector, \"z\")\n",
    "        angle_y = _find_angle(normal_vector, \"y\")\n",
    "        angle_x = _find_angle(normal_vector, \"x\")\n",
    "\n",
    "        # save the angle in an array\n",
    "        normal_angle[i] = angle\n",
    "        norm_ang_x[i] = angle_x\n",
    "        norm_ang_y[i] = angle_y\n",
    "        # save the vector in an array\n",
    "        normal_vectors[i] = normal_vector\n",
    "\n",
    "        # Find the eigenvalues of the matrix\n",
    "        eival1, eival2, eival3 = pca.singular_values_\n",
    "\n",
    "        # calculate the following values and assign them to the correct spot in the array\n",
    "        linearity[i] = (eival1 - eival2) / eival1\n",
    "        planarity[i] = (eival2 - eival3) / eival1\n",
    "        scattering[i] = eival3 / eival1\n",
    "        omnivariance[i] = (eival1 * eival2 * eival3) ** (1 / 3)\n",
    "        anisotropy[i] = (eival1 - eival3) / eival1\n",
    "        eigenentropy[i] = -1 * (\n",
    "            eival1 * np.log(eival1) + eival2 * np.log(eival2) + eival3 * np.log(eival3)\n",
    "        )\n",
    "        eigsum[i] = eival1 + eival2 + eival3\n",
    "        ch_cur[i] = eival3 / (eival1 + eival2 + eival3)\n",
    "\n",
    "    pca_geo_array = np.vstack(\n",
    "        [\n",
    "            normal_angle,\n",
    "            norm_ang_x,\n",
    "            norm_ang_y,\n",
    "            linearity,\n",
    "            planarity,\n",
    "            scattering,\n",
    "            omnivariance,\n",
    "            anisotropy,\n",
    "            eigenentropy,\n",
    "            eigsum,\n",
    "            ch_cur,\n",
    "        ]\n",
    "    ).T\n",
    "\n",
    "    return pca_geo_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the geoetric featuers\n",
    "# NOTE this cell is *slow* (~4minutes?)\n",
    "pca_results8 = pca_3d_shape(all_points, tree, k=8)\n",
    "pca_results15 = pca_3d_shape(all_points, tree, k=15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign the calculated angle to the new dimension created above\n",
    "subset.normal_angle = pca_results8[:, 0]\n",
    "subset.linearity15 = pca_results15[:, 3]\n",
    "subset.planarity15 = pca_results15[:, 4]\n",
    "subset.scat15 = pca_results15[:, 5]\n",
    "subset.curve15 = pca_results15[:, 10]\n",
    "\n",
    "# save the file\n",
    "subset.write(\"./data/subsetted_pk_code_20dec.las\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"subsetLoad\"></a>Loading subset from las file\n",
    "subset already has normals and other computations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = laspy.read(r\"./data/subsetted_pk_code_20dec.las\")\n",
    "# subset = laspy.read(r\"./data/fme_classifir.las\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in subset.point_format.dimension_names:\n",
    "    # need to handle different methods for views that return arrays and views that return subviews\n",
    "    try:\n",
    "        mean_ = np.array(subset[name]).mean()\n",
    "    except:\n",
    "        mean_ = \"N/A\"\n",
    "\n",
    "    print(f\"|{name}|{subset[name].max()}|{subset[name].min()}|{mean_}|  |\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data_dict = {}\n",
    "for i in subset.point_format.dimension_names:\n",
    "    if type(subset[i]) == laspy.point.dims.SubFieldView:\n",
    "        df_data_dict[i] = np.array(subset[i])\n",
    "    else:\n",
    "        df_data_dict[i] = subset[i]\n",
    "\n",
    "\n",
    "las_df = pd.DataFrame(data=df_data_dict)\n",
    "\n",
    "las_df.loc[:, \"x\"] = np.array(subset.x, dtype=np.float64)\n",
    "las_df.loc[:, \"y\"] = np.array(subset.y, dtype=np.float64)\n",
    "las_df.loc[:, \"z\"] = np.array(subset.z, dtype=np.float64)\n",
    "\n",
    "# las_df.loc[:,'XYZ_point'] = [Point(xyz) for xyz in list(zip( las_df.X, las_df.Y, las_df.Z  ))]\n",
    "las_df.loc[:, \"xy_point\"] = [Point(xyz) for xyz in list(zip(las_df.x, las_df.y))]\n",
    "\n",
    "las_df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the labelled shapefile\n",
    "labels to be attached to the points by spatial join<br>\n",
    "Labels are:\n",
    "| Class     | Value |\n",
    "|-----------|-------|\n",
    "| Trees     | 1     |\n",
    "| Grass     | 2     |\n",
    "| Buildings | 3     |\n",
    "| Sand      | 4     |\n",
    "| Road      | 5     |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fh = open()\n",
    "\n",
    "classDF = gpd.read_file(\"./data/labels_classes1.shp\", epsg=28992)\n",
    "classDF\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf = gpd.GeoDataFrame(las_df, crs=\"EPSG:28992\", geometry=las_df.xy_point)\n",
    "gdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined = gdf.sjoin(classDF, how=\"inner\")\n",
    "joined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( np.unique(joined.label) )\n",
    "joined.to_pickle('./data/classfied_df.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest\n",
    "## test train split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load from pkl file, the classified df\n",
    "labelled_df = pd.read_pickle('./data/classfied_df.pkl')\n",
    "labelled_df.drop(['x','y','z','xy_point', 'index_right',  'edge_of_flight_line', 'classification', 'user_data', 'point_source_id', 'gps_time', 'nir', 'synthetic', 'key_point', 'withheld', 'overlap', 'scanner_channel','scan_direction_flag'], inplace=True, axis=1)\n",
    "labelled_df.rename(columns={\"label_right\": \"class\"}, inplace=True)\n",
    "labelled_df = gpd.GeoDataFrame(labelled_df, crs=\"EPSG:28992\", geometry=labelled_df.geometry)\n",
    "\n",
    "labelled_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = labelled_df.loc[:, 'intensity':'curve15']  \n",
    "y =labelled_df['label']\n",
    "\n",
    "# uses a 75 25 split ratio\n",
    "X_scaled = StandardScaler().fit_transform(X)\n",
    "X_train, x_test, Y_train, y_test = train_test_split(X_scaled, y, random_state=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train\n",
    "# y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_dic = {\n",
    "          'RandomForestClassifier_10': RandomForestClassifier(n_estimators=10),\n",
    "          'RandomForestClassifier_50': RandomForestClassifier(n_estimators=50),\n",
    "          'RandomForestClassifier_100': RandomForestClassifier(n_estimators=100)\n",
    "          }\n",
    "\n",
    "\n",
    "def classification_func(classifier_dic, X_train, Y_train, x_test, y_test, X, y):   \n",
    "\n",
    "    for name,classifier in classifier_dic.items():\n",
    "        \n",
    "        print(f'/////////////////// heyyyy we are now classifying using \\t\\t {name} ')\n",
    "        \n",
    "        classifier.fit(X_train, Y_train)\n",
    "        pred = classifier.predict(x_test)\n",
    "        score = classifier.score(x_test, y_test)\n",
    "        print(pred)\n",
    "        print(score)\n",
    "        # scores = cross_val_score(classifier, X, y , scoring='accuracy', cv=7) #X needs scaling\n",
    "        # print(f\"Mean score: {scores.mean()}, Std dev: {scores.std()}\")\n",
    "\n",
    "\n",
    "        # print(f\"R2 train:{r2_score(Y_train, classifier.predict(X_train))}\")\n",
    "        # print(f\"R2 test :{r2_score(y_test,pred)}\")\n",
    "        # # print(f\"MSE:     {mean_squared_error(y_test,pred)}\")\n",
    "        # # print(f\"MAE:     {mean_absolute_error(y_test, pred)}\")\n",
    "        # print(f\"MAPE:    {mean_absolute_percentage_error(y_test, pred)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_func(classifier_dic,  X_train, Y_train, x_test, y_test, X_scaled, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### workflow:\n",
    "\n",
    "1. split the dataset into train and test\n",
    "1. use BAG for buildings, sand manually, tree manually and grass manually, road manually using predefined shapes (preferably made in QGIS, saved as geojson file)\n",
    "1. use the signatures for labelling the training dataset\n",
    "1. train random forest on training, use a dictionary with different param values as done for biomass estimation at space4good\n",
    "1. test classification prediction\n",
    "\n",
    "### TODO\n",
    "* fix df\n",
    "* make current bbox into a geojson, push to ./data\n",
    "* make training shapes\n",
    "* think of more params to make for classification\n",
    "* import code from random forest prediction of biomass for S4G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset.scan_direction_flag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking that the angles with the vertical makes sense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20, 20))\n",
    "ax = fig.add_subplot(111, projection=\"3d\")\n",
    "min_max_scaler = MinMaxScaler()\n",
    "color = min_max_scaler.fit_transform(\n",
    "    np.vstack([subset.red, subset.green, subset.blue]).T\n",
    ")\n",
    "\n",
    "p = ax.scatter(subset.X, subset.Y, subset.Z, s=1, c=subset.normal_angle)\n",
    "ax.set_zlim3d(bottom=subset.Z.min())\n",
    "\n",
    "plt.colorbar(p, shrink=0.75, label=\"Angle with the vertical axis\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO delete this cell later when subset is finalized\n",
    "# create a markdown table with max and min, to copy in to the cell below\n",
    "\n",
    "for name in subset.point_format.dimension_names:\n",
    "    # need to handle different methods for views that return arrays and views that return subviews\n",
    "    try:\n",
    "        mean_ = subset[name].mean()\n",
    "    except:\n",
    "        mean_ = \"N/A\"\n",
    "\n",
    "    print(f\"|{name}|{subset[name].max()}|{subset[name].min()}|{mean_}|  |\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Investigate the data set: describe each of its columns and assess the spread in values in each column. What is the meaning of each attribute?\n",
    "\n",
    "| Column name         | max                | min                    | average               | description |\n",
    "| ------------------- | ------------------ | ---------------------- | --------------------- | ----------- |\n",
    "| X                   | 506423             | 336425                 | 429292.5475783807     |             |\n",
    "| Y                   | 447831             | 347833                 | 394849.7615190117     |             |\n",
    "| Z                   | 45237              | 8403                   | 19564.184924570316    |             |\n",
    "| intensity           | 3402               | 64                     | 942.8189832865345     |             |\n",
    "| return_number       | 5                  | 1                      | 1.1828812172482248    |             |\n",
    "| number_of_returns   | 5                  | 1                      | 1.3674346752991253    |             |\n",
    "| synthetic           | 0                  | 0                      | 0.0                   |             |\n",
    "| key_point           | 0                  | 0                      | 0.0                   |             |\n",
    "| withheld            | 0                  | 0                      | 0.0                   |             |\n",
    "| overlap             | 0                  | 0                      | 0.0                   |             |\n",
    "| scanner_channel     | 0                  | 0                      | 0.0                   |             |\n",
    "| scan_direction_flag | 1                  | 0                      | 0.4810196197664228    |             |\n",
    "| edge_of_flight_line | 1                  | 0                      | 0.0002962191261423998 |             |\n",
    "| classification      | 0                  | 0                      | 0.0                   |             |\n",
    "| user_data           | 7                  | 6                      | 6.481019619766423     |             |\n",
    "| scan_angle          | -3000              | -4667                  | -3794.8262143866364   |             |\n",
    "| point_source_id     | 103                | 102                    | 102.55177239640155    |             |\n",
    "| gps_time            | 270040552.1816593  | 270039062.8002339      | 270039884.3703773     |             |\n",
    "| red                 | 65280              | 11776                  | 34933.534884552784    |             |\n",
    "| green               | 65280              | 12544                  | 34257.16123262926     |             |\n",
    "| blue                | 65280              | 9984                   | 31645.35646450736     |             |\n",
    "| nir                 | 0                  | 0                      | 0.0                   |             |\n",
    "| Deviation           | 65535.0            | 0.0                    | 603.9157977572299     |             |\n",
    "| Reflectance         | 19.290000915527344 | -14.789999961853027    | -5.444933312188677    |             |\n",
    "| Amplitude           | 34.02000045776367  | 0.6399999856948853     | 9.42818983316471      |             |\n",
    "| normal_angle        | 89.99967179015843  | 0.015725464844876056   | 21.851322488186963    |             |\n",
    "| linearity15         | 0.926218614188531  | 0.00019597302723526418 | 0.2134752744229102    |             |\n",
    "| planarity15         | 0.9706762822613131 | 0.001229676703891111   | 0.5955540106253252    |             |\n",
    "| scat15              | 0.9484001216640616 | 0.00985768059600219    | 0.19097071495176482   |             |\n",
    "| curve15             | 0.3247035997146174 | 0.0060505239029392255  | 0.09341443281745175   |             |\n",
    "\n",
    "## Spread in RBG values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(subset.green, bins=20, color=\"green\", histtype=\"step\")\n",
    "plt.hist(subset.red, bins=20, color=\"red\", histtype=\"step\")\n",
    "plt.hist(subset.blue, bins=20, color=\"blue\", histtype=\"step\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) What are ways to create a subset? Select a suitable subset of the data and visualize it in 3D. Start small, and if computer and software permits, try a bit larger subset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Identify at least four different object classes. Choose your classes such that together these cover a large majority of your points.\n",
    "\n",
    "1. building\n",
    "2. grass\n",
    "3. pavement\n",
    "4. tree\n",
    "5. beach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d) Analyse which spatial scales and spectral properties are useful to distinguish your classes.\n",
    "\n",
    "The following properties are typical of each of the classes\n",
    "\n",
    "1. building\n",
    "   1. taller than the surroundings \n",
    "2. grass\n",
    "   1. high NDVI\n",
    "   2. \n",
    "3. pavement\n",
    "   1. \n",
    "4. tree\n",
    "   1. taller that it is wide or long\n",
    "5. beach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e) Extract training data for each of your object classes. Divide your training data in two parts, one for training, and one for validation. What could be the influence of  imbalances in your training data? How could your division in training and validation data affect your results?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "f) Find a suitable implementation of the Random Forest algorithm. What are its parameters? What would be good settings for these parameters, given your classification task? Apply Random Forest on your data using only your observed features, to make sure your setup is correct.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questions\n",
    "## Describe the properties of the data (sub)set that you will classify: \n",
    "### How many points? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The selected subset contains {len(subset)} points\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What area does it cover? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    f\"Bounding box used to create the subset is between {xmin} and {xmax} longitude and {ymin} and {ymax} latitude (referenced to the coordinates in the LAZ file [EPSG:28992])\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### What observed features will you use? \n",
    "### Visualize your final subset, including the useful observed features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12, 12))\n",
    "ax = fig.add_subplot(111, projection=\"3d\")\n",
    "min_max_scaler = MinMaxScaler()\n",
    "color = min_max_scaler.fit_transform(\n",
    "    np.vstack([subset.red, subset.green, subset.blue]).T\n",
    ")\n",
    "\n",
    "\n",
    "print(color.shape)\n",
    "ax.scatter(subset.X, subset.Y, subset.Z, s=1, c=color)\n",
    "ax.set_zlim3d(bottom=subset.Z.min())\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Describe your at least four different object classes; \n",
    "\n",
    "For each point determine neighborhoods of k1, k2, ... points (using e.g. an efficient data structure), and use these k1, k2, ... points to estimate several feature values.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Describe $\\gt$ 20 different geometric attributes, obtained using at least 2 different neighborhood sizes, to characterize your points. What are the dimensions of the data covariance matrix you use to compute the geometric features? Give one example on how you determine the PCA eigenvalues of one k-neighborhood. Indicate for each feature how it could help to distinguish your classes, given also the neighborhood sizes you consider.**\n",
    "\n",
    "The geometric attributes were computed using a covariance matrix in 3D. To describe the geometry of real world features, fitting the PCA plane in 3D makes the most sense. For every point in the subset, the nearest *k* points were found using a KDtree, which is an efficient data structure for finding the nearest features in 3D space. When working with such a large cloud, the computational expense is very important to be practical to run on a personal computer. Once the k nearest neighbors are found, a PCA model is trained on the XYZ location of the k-neighborhood, which returns 3 vectors (the eigenvectors of the Covariance matrix). The three vectors point in direction of the most variance, the 2nd most variance, and normal to the other two, respectively. The trained PCA model also returns the eigenvalues covariance matrix, which can be used to compute the following features:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Compute all geometric features for all points in your subset using Python. Visualize selected results, e.g. by combining features in a false color visualization and/or using histograms. Which features are best at discriminating your classes? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fc_vis_and_hist(point_cloud_subset, geometric_array, name):\n",
    "    # create a 3d visualization\n",
    "    fig = plt.figure(figsize=(24, 10))\n",
    "    ax = fig.add_subplot(1, 2, 1, projection=\"3d\")\n",
    "\n",
    "    assert len(point_cloud_subset) == len(geometric_array)\n",
    "\n",
    "    p = ax.scatter(subset.X, subset.Y, subset.Z, s=1, c=geometric_array)\n",
    "    plt.colorbar(p, shrink=0.75, label=name)\n",
    "    ax.set_zlim3d(bottom=subset.Z.min())\n",
    "    # plt.title(name)\n",
    "\n",
    "    # plt.show()\n",
    "\n",
    "    # create a histogram\n",
    "    # plt.figure(figsize=(12,7))\n",
    "    ax = fig.add_subplot(1, 2, 2)\n",
    "\n",
    "    ax.hist(geometric_array, bins=20)\n",
    "    plt.suptitle(name)\n",
    "    plt.xlabel(f\"values of {name}\")\n",
    "    plt.ylabel(\"Count of points\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geometric_feat_names = [\n",
    "    \"Normal Angle\",\n",
    "    \"Angle to X axis\",\n",
    "    \"Angle to Y axis\",\n",
    "    \"Linearity\",\n",
    "    \"Planarity\",\n",
    "    \"Scattering\",\n",
    "    \"Omnivariance\",\n",
    "    \"Anisotropy\",\n",
    "    \"Eigen Entropy\",\n",
    "    \"Eigen Sum\",\n",
    "    \"Change in Curvature\",\n",
    "]\n",
    "\n",
    "\n",
    "for column8, column15, name in zip(\n",
    "    pca_results8.T, pca_results15.T, geometric_feat_names\n",
    "):\n",
    "    fc_vis_and_hist(subset, column8, name + \"- k=8\")\n",
    "    fc_vis_and_hist(subset, column15, name + \"- k=15\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Describe and visualize your training data. Is your training data balanced? Make sure that a zone of your point cloud data is really ’unseen’, that is that no training data is taken from that zone, so you can inspect if Random Forest also works there.\n",
    "\n",
    "The training data is a set of 495,213 points that was created by manually creating polygons representing some desired classes, and the finding which points lie within those polygons when projected onto the 2d plane. The final count of training points for each category is:\n",
    "\n",
    "| Class     | Number of points |\n",
    "| --------- | ---------------- |\n",
    "| Trees     | 190,242          |\n",
    "| Grass     | 105,998          |\n",
    "| Buildings | 910,39           |\n",
    "| Sand      | 14,489           |\n",
    "| Road      | 93,455           |\n",
    "\n",
    "The selected training data is shown in the figure below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12, 12))\n",
    "ax = fig.add_subplot(111, projection=\"3d\")\n",
    "\n",
    "for i,name in enumerate(['Trees','Grass','Buildings','Sand','Road']):\n",
    "    set = labelled_df[labelled_df.label==i+1]\n",
    "    ax.scatter(set.X,set.Y,set.Z,label=name,s=0.5)\n",
    "plt.title('Labeled Training data')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Feed your training data to Random Forest using at least 10 of your best geometric and observed features. What settings did you use?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Classify your point cloud data and visualize and discuss your result. What went well? Give also examples where the classifier mixed up classes. What are possible explanations for these confusions? How are the classification results on the unseen zone?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5e58f0493970c85668badbc5bf106a4a788c4b99d38880c90ea927a02d72e3dc"
  },
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit ('BPD4a': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
